{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Data Preparation"
      ],
      "metadata": {
        "id": "0-wvnd4d-9JY"
      },
      "id": "0-wvnd4d-9JY"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install emoji"
      ],
      "metadata": {
        "id": "JmewqtoHugMn"
      },
      "id": "JmewqtoHugMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb24d23",
      "metadata": {
        "id": "3cb24d23"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import emoji\n",
        "import re\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "sns.set_style('darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qMn7EwNOjqE3"
      },
      "id": "qMn7EwNOjqE3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtendo os dados"
      ],
      "metadata": {
        "id": "7ux7GYfl_CaB"
      },
      "id": "7ux7GYfl_CaB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b9efa6",
      "metadata": {
        "id": "d6b9efa6"
      },
      "outputs": [],
      "source": [
        "positives = twitter_samples.strings('positive_tweets.json')\n",
        "negatives = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(\"Positivos: \", len(positives), \"Negativos: \", len(negatives))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estatísticas Iniciais"
      ],
      "metadata": {
        "id": "n8C4FHIV7xuI"
      },
      "id": "n8C4FHIV7xuI"
    },
    {
      "cell_type": "markdown",
      "id": "144bb3d5",
      "metadata": {
        "id": "144bb3d5"
      },
      "source": [
        "Neste exercício, temos uma base com 10 mil tweets, entre positivos e negativos. Imaginando que vocês irão utilizar esta base para fazer análise de sentimentos, neste exercícios vocês devem fazer a etapa de pré-processamento de dados.\n",
        "\n",
        "Como dito em aula, o pré-processamento é um estágio de tomada de decisões, então vocês vão ter que pensar sobre várias decisões, como por exemplo:\n",
        "\n",
        " - Tokenização: Tratar todos os símbolos como separadores? E números? Tudo em caixa baixa?\n",
        " - Hashtags e Mentions: O que fazer com eles? Tratar como texto? Guardar separado? E as hashtags em camelcase? E os RTs?\n",
        " - URLs: Vão ser jogadas fora, substituidas por um token genérico ou transformadas em texto?\n",
        " - Remover Stopwords? Lemmatization? Stemming?\n",
        " - Outras técnicas de remover ruído dos tokens?\n",
        " - Correção de erros de digitação? Com quais limiares?\n",
        " - Que outras limpezas devem ser feitas?\n",
        " - Como definir o tamanho final do vocabulário a ser considerado e quais palavras estarão nele? (classificadores trabalham com conjuntos finitos de palavras, como veremos em aulas futuras.\n",
        " - O que fazer com emojis.\n",
        "\n",
        "\n",
        "O que entregar:\n",
        "\n",
        "Este notebook preenchido com a sua exploração desta base de dados, o código para o pre-processamento e, mais importante, as __justificativas__ para as decisões de pré-processamento que vocês estão tomando.\n",
        "\n",
        "Estatísticas do pré-processamento que também devem ser apresentadas (e serão úteis para as decisões):\n",
        "\n",
        " - Total de sentenças. ✅\n",
        " - Total de palavras antes e depois do pre-processamento. ✅\n",
        " - Total de termos únicos antes e depois. ✅\n",
        " - Tamanho do vocabulário final.\n",
        " - Média de palavras por tweet e por sentença. ✅\n",
        " - Plot de frequência das Top 20 palavras mais comuns. ✅\n",
        " - Lista de palavras mais longas. ✅\n",
        " - Total de palavras todas em maiúsculo. ✅\n",
        " - Total de RTs ✅\n",
        " - Total de usuários mencionados e média/tweet. ✅"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "tweets = []\n",
        "# remoção de pontuação usando `maketrans` acaba concatenando palavras separadas por vírgula\n",
        "tweets.extend(positives)\n",
        "tweets.extend(negatives)\n",
        "\n",
        "# calculando o total de sentenças\n",
        "sentences = []\n",
        "for tweet in tweets:\n",
        "    sentences.extend(tweet.split('\\n'))\n",
        "print(f'Total de sentenças: {len(sentences)}')\n",
        "\n",
        "# calculando a fequência de distribuição de tokens\n",
        "tokens = nltk.word_tokenize(' '.join(tweets))\n",
        "freqs = nltk.FreqDist(tokens)\n",
        "freqtot = sum(freqs.values())\n",
        "\n",
        "# vocabulário inicial é igual ao total de entradas (chaves) no `FreqDist`\n",
        "print(f'Vocabulário inicial: {len(freqs.keys())}')\n",
        "\n",
        "# calculando a fequência de distribuição de tokens em maiúsuclo\n",
        "tokens = nltk.word_tokenize(' '.join(tweets).upper())\n",
        "freqs = nltk.FreqDist(tokens)\n",
        "\n",
        "# removendo tokens de pontuação\n",
        "freqs = dict([(k,freqs[k]) for k in freqs if k.isalnum()])\n",
        "\n",
        "# calculando o vocabulário total\n",
        "freqtot = sum(freqs.values())\n",
        "\n",
        "# vocabulário inicial é igual ao total de entradas (chaves) no `FreqDist`\n",
        "print(f'Vocabulário inicial maiúsculo: {len(freqs.keys())}')\n",
        "\n",
        "# a quantidade total de palavras é o somatório das frequências em `FreqDist`\n",
        "total_tokens = sum([freqs[k] for k in freqs])\n",
        "print(f'Total de tokens: {total_tokens}')\n",
        "print(f'Média de tokens por tweet: {freqtot / len(tweets):.2f}')\n",
        "print(f'Média de tokens por sentença: {freqtot / len(sentences):.2f}')"
      ],
      "metadata": {
        "id": "2bf7GVQ-3V0b"
      },
      "id": "2bf7GVQ-3V0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ordenando decrescente por frequencia\n",
        "freqs_sorted = sorted(freqs, key=freqs.get, reverse=True)\n",
        "\n",
        "print('Lista das 20 palavras mais frequentes')\n",
        "for freq in freqs_sorted[:20]:\n",
        "    print(' ', freq, '\\t', freqs[freq])\n"
      ],
      "metadata": {
        "id": "nyMftIhT7waP"
      },
      "id": "nyMftIhT7waP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Top 20 palavras mais frequentes no corpus')\n",
        "sns.barplot(y=[k for k in freqs_sorted[:20]], x=[freqs[k] for k in freqs_sorted[:20]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CsunrHua--uN"
      },
      "id": "CsunrHua--uN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_sorted = sorted(freqs.keys(), key=len, reverse=True)\n",
        "\n",
        "print('Lista das 20 palavras mais longas')\n",
        "for word in len_sorted[:20]:\n",
        "    print(' ', len(word), '\\t', word)"
      ],
      "metadata": {
        "id": "mDy-8p4XCCKA"
      },
      "id": "mDy-8p4XCCKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Top 20 palavras mais longas')\n",
        "sns.barplot(y=[k for k in len_sorted[:20]], x=[len(k) for k in len_sorted[:20]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "deQEfjYDHJME"
      },
      "id": "deQEfjYDHJME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpeza inicial"
      ],
      "metadata": {
        "id": "EUEMTvQI_H1V"
      },
      "id": "EUEMTvQI_H1V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decidi começar removendo URLs pois não vejo sentindo para o contexto de análise de sentimentos."
      ],
      "metadata": {
        "id": "LPCF0HpU88qb"
      },
      "id": "LPCF0HpU88qb"
    },
    {
      "cell_type": "code",
      "source": [
        "url_pattern1 = re.compile('https?:\\/\\/www[a-zA-Z0-9\\/\\.]+')\n",
        "url_pattern2 = re.compile('[@a-zA-Z0-9\\.\\_]+\\.com[@a-zA-Z0-9\\.\\_]?')\n",
        "twitter_url_pattern = re.compile('https?:\\/\\/t\\.co\\/[a-zA-Z0-9]{8,10}')\n",
        "removed = set()\n",
        "\n",
        "def remove_urls(tweet: str):\n",
        "    urls = twitter_url_pattern.findall(tweet)\n",
        "    for url in urls:\n",
        "        removed.add(url)\n",
        "        tweet = tweet.replace(url, ' ')\n",
        "\n",
        "    urls = url_pattern1.findall(tweet)\n",
        "    for url in urls:\n",
        "        removed.add(url)\n",
        "        tweet = tweet.replace(url, ' ')\n",
        "\n",
        "    urls = url_pattern2.findall(tweet)\n",
        "    for url in urls:\n",
        "        removed.add(url)\n",
        "        tweet = tweet.replace(url, ' ')\n",
        "\n",
        "    return tweet\n",
        "\n",
        "tweets_clean = [remove_urls(tweet) for tweet in tweets]\n",
        "\n",
        "for r in removed:\n",
        "    print(r)"
      ],
      "metadata": {
        "id": "BUFwh9un9DrZ"
      },
      "id": "BUFwh9un9DrZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in ['.com','.br','.tv','youtube.com', 'twitch.', 't.co']:\n",
        "    for t in tweets_clean:\n",
        "        if k in t:\n",
        "            print(t)"
      ],
      "metadata": {
        "id": "EYjMG3rzTR8q"
      },
      "id": "EYjMG3rzTR8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Quantidade de URLs distintas removidas: {len(removed)}')"
      ],
      "metadata": {
        "id": "clR15eWcAuLC"
      },
      "id": "clR15eWcAuLC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(len(tweets_clean) == 10 ** 4)"
      ],
      "metadata": {
        "id": "BPKPVak_Azfh"
      },
      "id": "BPKPVak_Azfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora começo a limpeza do texto utilizando os seguintes passos:\n",
        "\n",
        "1. Converter o format `web` do tweet para `texto` usando `BeautifulSoup`, isso é necessário para trocar por exemplo a codificação `%amp;` por `&`. Isso irá facilitar a remoção de pontuação do texto.\n",
        "1. Remover as quebras de linha `\\n` dos tweets.\n",
        "1. Substituir ocorrências consecutivas de sinais de pontução, como por exemplo `!!!!!!!` por apenas `!`. Isso reduz os `tokens` e irá facilitar tanto a remoção de pontuação quanto a identificação de `emotes`, pois desse modo os emotes `:))))))))` e `:)))` serão padronizados como `:)`.\n",
        "1. Por fim o tweet é convertido para `minúsculo`."
      ],
      "metadata": {
        "id": "Xo2qsCL-A92J"
      },
      "id": "Xo2qsCL-A92J"
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def prepare_tweet(tweet: str, remove_duplicate_punct=False):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    tweet = tweet.expandtabs().replace('\\n', ' ')\n",
        "    if remove_duplicate_punct:\n",
        "        tweet = re.sub('!+', '!', tweet)\n",
        "        tweet = re.sub('\\.+', '.', tweet)\n",
        "        tweet = re.sub('\\)+', ')', tweet)\n",
        "        tweet = re.sub('\\(+', '(', tweet)\n",
        "        tweet = re.sub(\"'+\", \"'\", tweet)\n",
        "        tweet = re.sub(\"`+\", \"`\", tweet)\n",
        "        tweet = re.sub(r\"-+\", r\"-\", tweet)\n",
        "        tweet = re.sub(\"/+\", \"/\", tweet)\n",
        "        tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet\n",
        "\n",
        "tweets_clean = [prepare_tweet(tweet, True) for tweet in tweets_clean]\n",
        "\n",
        "assert(len(tweets_clean) == 10 ** 4)\n",
        "\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "bSet6IOejz6L"
      },
      "id": "bSet6IOejz6L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hashtags\n"
      ],
      "metadata": {
        "id": "9rsn1la6YHmO"
      },
      "id": "9rsn1la6YHmO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "TVu5wGqeR0be"
      },
      "id": "TVu5wGqeR0be"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "hashtag_pattern = re.compile('#[a-zA-Z\\-]+')\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for t in tweets_clean:\n",
        "    rs = hashtag_pattern.findall(t)\n",
        "    if rs:\n",
        "        counter.update(rs)\n",
        "\n",
        "print(f'Quantidade de hashtags únicas: {len(counter.keys())}')\n",
        "print(f'Total de hashtags: {sum(counter.values())}')\n",
        "print(f'Média de hashtags por Tweet: {sum(counter.values()) / len(tweets_clean)}')"
      ],
      "metadata": {
        "id": "7_S1_bDdYI-7"
      },
      "id": "7_S1_bDdYI-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ordenando decrescente por frequencia\n",
        "tags_sorted = sorted(counter, key=counter.get, reverse=True)\n",
        "\n",
        "print('Lista das 20 hashtags mais frequentes')\n",
        "for tag in tags_sorted[:20]:\n",
        "    print(counter[tag], '\\t', tag)"
      ],
      "metadata": {
        "id": "48QxlpqLU7ML"
      },
      "id": "48QxlpqLU7ML",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Top 20 hastags mais comuns')\n",
        "sns.barplot(y=[tag for tag in tags_sorted[:20]], x=[counter[tag] for tag in tags_sorted[:20]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S3pOwY74VYIR"
      },
      "id": "S3pOwY74VYIR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acredito que as tags contenham informações relevantes para um modelo de análise sentimental, sendo assim irei tratar as Hashtags usando a seguinte abordagem:\n",
        "\n",
        "- Remover o `#`\n",
        "- As hashtags em `PascalCase` serão splitadas em palavras separadas, como por exemplo `#BraindDots` resultará em `Brain Dots`"
      ],
      "metadata": {
        "id": "wmFk4RW0XZAp"
      },
      "id": "wmFk4RW0XZAp"
    },
    {
      "cell_type": "code",
      "source": [
        "def case_split(identifier):\n",
        "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
        "    return ' '.join([m.group(0).strip() for m in matches])\n",
        "\n",
        "def prepare_hashtag(tweet: str):\n",
        "    tags = hashtag_pattern.findall(tweet)\n",
        "    for tag in tags:\n",
        "        tweet = tweet.replace(tag, case_split(tag.replace('#', '')))\n",
        "    return tweet\n",
        "\n",
        "tweets_clean = [prepare_hashtag(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "tJDb5yHiWBl6"
      },
      "id": "tJDb5yHiWBl6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usuários mencionados\n",
        "\n",
        "- Segundo a página [Helo with your username](https://help.twitter.com/en/managing-your-account/twitter-username-rules#:~:text=Your%20username%20cannot%20be%20longer,of%20underscores%2C%20as%20noted%20above.) os nomes de usuários no Twitter devem ter até 15 caracteres do conjunto `[A-Z0-9_]`.\n",
        "\n",
        "- Fiz uma busca na internet e não encontrei artigos que dessem orientações quanto ao pré-processamento de `usernames`. Sendo assim minha ideia é de que essa informação não seja relevante para tarefas de `análise de sentimentos`."
      ],
      "metadata": {
        "id": "XNfRfe2es-Tb"
      },
      "id": "XNfRfe2es-Tb"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "username_pattern = re.compile('@[A-Z0-9_]+', re.IGNORECASE)\n",
        "\n",
        "mentions = []\n",
        "\n",
        "for tweet in tweets_clean:\n",
        "    usernames = username_pattern.findall(tweet)\n",
        "    if usernames:\n",
        "        mentions.extend(usernames)\n",
        "\n",
        "contador = Counter(mentions)\n",
        "\n",
        "print(f'Total menções a usuários: {sum(contador.values())}')\n",
        "print(f'Total usuários mencionados: {len(contador.keys())}')\n",
        "print(f'Média de menções por tweet: {sum(contador.values()) / len(tweets_clean)}')"
      ],
      "metadata": {
        "id": "4ybh7zj7tCTi"
      },
      "id": "4ybh7zj7tCTi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top 20 usuários mais mencionados\n",
        "\n",
        "- Existem muitas menções a artistas.\n",
        "- Não existe muito padrão nos nomes de usuários."
      ],
      "metadata": {
        "id": "0-4oUqrK5Z8U"
      },
      "id": "0-4oUqrK5Z8U"
    },
    {
      "cell_type": "code",
      "source": [
        "df_mentions = pd.DataFrame(contador.items(), columns=['username', 'n_mentions'])\n",
        "df_mentions.sort_values(by='n_mentions', ascending=False).head(20)"
      ],
      "metadata": {
        "id": "VbIt9aMp5R1C"
      },
      "id": "VbIt9aMp5R1C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=df_mentions.sort_values(by='n_mentions', ascending=False).head(20), y='username', x='n_mentions')\n",
        "# plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DsnHFhXp5WQt"
      },
      "id": "DsnHFhXp5WQt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_mentions(tweet: str):\n",
        "    results = username_pattern.findall(tweet)\n",
        "    for name in results:\n",
        "        tweet = tweet.replace(name, ' ')\n",
        "\n",
        "    return tweet\n",
        "\n",
        "tweets_clean = [prepare_mentions(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "5_3jvKMF5xMt"
      },
      "id": "5_3jvKMF5xMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emojis e Emotes\n",
        "- O tutorial de [Bael Chen](https://towardsdatascience.com/emojis-aid-social-media-sentiment-analysis-stop-cleaning-them-out-bb32a1e5fc8e) demonstra brevemente que alguns modelos (independente do método de tokenização utilizado) podem trazer resultados melhores quando so `emojis` são mantidos no testo.\n",
        "\n",
        "- O artigo [Sentiment of Emojis, Petra Novak et. al.](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0144296&type=printable) faz uma análise da relevência dos Emojis para análise sentimental.\n",
        "\n",
        "- Sendo assim, decidi manter os `emojis` no texto, porém realizarei uma substituição por tags mais explicativas, por exemplo o emoji 😀 será substituído por `smile`.\n",
        "\n",
        "- Os `emotes` (`^.^` por exemplo) são muito variáveis e não apresentam uma formatação tão lógica, sendo assim serão removidos junto com a `pontuação`.\n"
      ],
      "metadata": {
        "id": "JBRIYwnFfPEP"
      },
      "id": "JBRIYwnFfPEP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estatísticas"
      ],
      "metadata": {
        "id": "3j4ooYXwRv2L"
      },
      "id": "3j4ooYXwRv2L"
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_freq = [emoji.emoji_count(tweet) for tweet in tweets_clean]\n",
        "emoji_freq = np.array(emoji_freq)\n",
        "\n",
        "print(f\"{'ANÁLISE ESTATÍSTICA DE EMOJIS':-^60}\")\n",
        "\n",
        "print(f'{\"Freq Acum\":^10}|{\"MAX Freq\":^10}|{\"MIN Freq\":^10}|{\"Mean\":^18}|{\"Median\":^10}')\n",
        "print(f'{emoji_freq.sum():^10}|{emoji_freq.max():^10}|{emoji_freq.min():^10}|{f\"{emoji_freq.mean()} (+-{emoji_freq.std():.2f})\":^18}|{np.median(emoji_freq):^10}')\n",
        "print('-'*60)\n"
      ],
      "metadata": {
        "id": "IZWlkKwqmXnC"
      },
      "id": "IZWlkKwqmXnC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "plt.title('Histograma de Frequência de Emojis')\n",
        "sns.histplot(emoji_freq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yLrpgFbOnWg2"
      },
      "id": "yLrpgFbOnWg2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "plt.title('Boxplot de Frequência de Emojis')\n",
        "sns.boxplot(x=emoji_freq)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mG1JZ5jjozpZ"
      },
      "id": "mG1JZ5jjozpZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'q1={np.quantile(emoji_freq, .25)}\\nq2={np.quantile(emoji_freq, .5)}\\nq3={np.quantile(emoji_freq, .75)}')"
      ],
      "metadata": {
        "id": "ur073e-5rxxu"
      },
      "id": "ur073e-5rxxu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_emojis(text:str, remove_emojis=False):\n",
        "    if remove_emojis:\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "def prepare_emoji(tweet: str, remove_emojis=False):\n",
        "    tweet = process_emojis(tweet, remove_emojis) # traduz emojis\n",
        "    tweet = tweet.replace(':', ' ') # separa emojis consecutivos\n",
        "    return tweet.lower()"
      ],
      "metadata": {
        "id": "mhDJ2lUQ-7C7"
      },
      "id": "mhDJ2lUQ-7C7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_clean = [prepare_emoji(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "FAi159N5OW9K"
      },
      "id": "FAi159N5OW9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pontuação\n",
        "\n",
        "Os caracteres de pontuação serão removidos."
      ],
      "metadata": {
        "id": "EA6_f0iUhrrq"
      },
      "id": "EA6_f0iUhrrq"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_punct(tweet: str):\n",
        "    new_tweet = ''\n",
        "    for c in tweet:\n",
        "        if c.isalpha():\n",
        "            new_tweet += c\n",
        "        else:\n",
        "            new_tweet += ' '\n",
        "    return re.sub(' +', ' ', new_tweet).strip()\n",
        "\n",
        "tweets_clean = [prepare_punct(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "RFWQVasFkeX6"
      },
      "id": "RFWQVasFkeX6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correção gramatical\n",
        "\n",
        "Como os tweets são proveninentes de uma ambiente web, é esperado que os usuário não respeitem as normas gramaticais da linguagem, o que pode atrapalhar a análise sentimental pois na lingua inglesa até a simples alteração na ordem de uma palavra pode alterar seu sentido.\n",
        "\n",
        "Por isso, tentei aplicar o modelo de correção gramatical `happytransformer` nos tweets, contudo não houveram modificações significativas nos textos.\n",
        "\n",
        "Além disso, também testei as abordagens clássicas utilizando `distância de edição mínima` e `distância de jaccard`. Ocorreu que muitas vezes estas resultam em palavras fora do contexto.\n",
        "\n",
        "Sendo assim, adotei apenas algumas correções `heurísticas`"
      ],
      "metadata": {
        "id": "TUVNx2XYpPQK"
      },
      "id": "TUVNx2XYpPQK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Substituir algumas contrações, comuns na língua inglesa, pela forma estendida"
      ],
      "metadata": {
        "id": "8qt4GVBIVUSx"
      },
      "id": "8qt4GVBIVUSx"
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_contractions(tweet: str):\n",
        "    tweet = re.sub(r\"(are|do|did|does|would|have|can|could|should|had|was)n?('|\\s)t\", r\"\\1 not\", tweet)\n",
        "    tweet = re.sub(r\"(she|he|it|they|what|that|where|there|who|here|when|how)('|\\s)s\", r\"\\1 is\", tweet)\n",
        "    tweet = re.sub(r\"(i|they|what|that|where|there|who|here|when|how)('|\\s)s\", r\"\\1 is\", tweet)\n",
        "    return tweet\n",
        "\n",
        "tweets_clean = [extend_contractions(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "HkPh7FhU0Ufp"
      },
      "id": "HkPh7FhU0Ufp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Também construí um `dict`com gírias encontradas na base e irei convertê-las para seu significado."
      ],
      "metadata": {
        "id": "0v0VCPUkXUey"
      },
      "id": "0v0VCPUkXUey"
    },
    {
      "cell_type": "code",
      "source": [
        "slangs = {\n",
        "    'u': 'you',\n",
        "    'y': 'you',\n",
        "    \"bffs\": 'best friends forever',\n",
        "    'bff': 'best friends forever',\n",
        "    'bf': 'best friend',\n",
        "    \"gfs\": 'girlfriends',\n",
        "    \"oomf\": 'one of my friends',\n",
        "    'smh': 'shake my head',\n",
        "    'lolz': 'laughing out loud',\n",
        "    'lols': 'laughing out loud',\n",
        "    'lol': 'laughing out loud',\n",
        "    'ofc': 'of course',\n",
        "    'utd': 'up to date',\n",
        "    'tmr': 'tomorrow',\n",
        "    'tmrw': 'tomorrow',\n",
        "    'omfg': 'oh my fucking god',\n",
        "    'omf': 'oh my god',\n",
        "    'xmas': 'christmas',\n",
        "    'wat': 'what',\n",
        "    'wtf': 'what that fuck',\n",
        "    'dm': 'direct message',\n",
        "    'dms': 'direct messages',\n",
        "    'wth': 'what that hell',\n",
        "    'imma': 'i am going to',\n",
        "    'ill': 'i will',\n",
        "    'txt': 'text',\n",
        "    'dat': 'that',\n",
        "    'rofl': 'rolled on the floor laughing',\n",
        "    'istg': 'i swear to god',\n",
        "    'btwn': 'between',\n",
        "    'tfw': 'that feel when',\n",
        "    'msg': 'message',\n",
        "    'msgs': 'messages',\n",
        "    'arent': 'are not',\n",
        "    'tbf': 'to be fair',\n",
        "    'rtss': 'retweets',\n",
        "    'ikr': 'i know right',\n",
        "    'yrs': 'years',\n",
        "    'nw': 'no worries',\n",
        "    'prob': 'probally',\n",
        "    'lmao': 'laughing my ass off',\n",
        "    'lmaoo': 'laughing my ass off',\n",
        "    'gotta': 'got to',\n",
        "    'hopeyou': 'hope you',\n",
        "    'nxt': 'next',\n",
        "    'ftw': 'for the win',\n",
        "    'brb': 'be right back',\n",
        "    'img': 'image',\n",
        "    'feb': 'february',\n",
        "    'fnaf': \"five night's at freddy's\",\n",
        "    'gl': 'good luck',\n",
        "    'gn': 'good night',\n",
        "    'ty': 'thank you',\n",
        "    'cya': 'see you all',\n",
        "    'ttyl': 'talk to you later',\n",
        "    'plz': 'please',\n",
        "    'pls': 'please',\n",
        "    'ffs': 'for fucks sake',\n",
        "    'tkts': 'tickets',\n",
        "    'fyi': 'for your information',\n",
        "    'otw': 'on the way',\n",
        "    'fck': 'fuck',\n",
        "    'sry': 'sorry',\n",
        "    'lyf': 'love you forever',\n",
        "    'luv': 'love',\n",
        "    'nvm': 'nevermind',\n",
        "    'r': 'are',\n",
        "    'cuz': 'cause',\n",
        "    'fb': 'facebook',\n",
        "    'ur': 'you are',\n",
        "    'rply': 'reply',\n",
        "    'knw': 'know',\n",
        "    'rly': 'really',\n",
        "    'fany': 'funny',\n",
        "    'atty': 'attorney',\n",
        "    'alrd': 'already',\n",
        "    'pic': 'picture',\n",
        "    'pics': 'pictures',\n",
        "    'otp': 'one true pairing',\n",
        "    'bday': 'birthday',\n",
        "    'lil': 'little',\n",
        "    'cz': 'cause',\n",
        "    'ppl': 'people',\n",
        "    'ytb': 'youtube',\n",
        "    'tym': 'thank you soo much',\n",
        "    'plss': 'please',\n",
        "    'lils': 'littles',\n",
        "    'tdy': 'today',\n",
        "    'fav': 'favorite'\n",
        "}"
      ],
      "metadata": {
        "id": "xCPJvErKznED"
      },
      "id": "xCPJvErKznED",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_slang(tweet: str):\n",
        "    return ' '.join([slangs.get(word, word) for word in tweet.split(' ')])\n",
        "\n",
        "tweets_clean = [replace_slang(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "FmrL56k_Y7c2"
      },
      "id": "FmrL56k_Y7c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.metrics.distance import edit_distance, jaccard_distance\n",
        "\n",
        "# nltk.download('words')\n",
        "# from nltk.corpus import words\n",
        "# correct_words = words.words()\n",
        "\n",
        "# tweets2 = [t.lower() for t in tweets_clean]\n",
        "\n",
        "# c = set()\n",
        "# for t in tweets2:\n",
        "#     for w in t.split(' '):\n",
        "#         if not w in correct_words):\n",
        "#             c.add(w)\n",
        "\n",
        "# print(len(c))\n",
        "# print(c)"
      ],
      "metadata": {
        "id": "-RWA9iPYzNrv"
      },
      "id": "-RWA9iPYzNrv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.metrics.distance import jaccard_distance\n",
        "# from nltk.util import ngrams\n",
        "\n",
        "# incorrect_words=['ystrday', 'crazyyyy', 'selamat']\n",
        "\n",
        "# for word in incorrect_words:\n",
        "#     temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
        "#                               set(ngrams(w, 2))),w)\n",
        "#             for w in correct_words if w[0]==word[0]]\n",
        "#     print(sorted(temp, key = lambda val:val[0])[0][1])"
      ],
      "metadata": {
        "id": "qhJcE7cMFR7a"
      },
      "id": "qhJcE7cMFR7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # !pip install happytransformer\n",
        "# from happytransformer import HappyTextToText, TTSettings\n",
        "\n",
        "# happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
        "# args = TTSettings(num_beams=5, min_length=1)\n",
        "\n",
        "# def prepare_grammar(tweet: str):\n",
        "#     # Add the prefix \"grammar: \" before each input\n",
        "#     result = happy_tt.generate_text(f\"grammar: {tweet}.\", args=args)\n",
        "#     return result.text\n",
        "\n",
        "# for i in [810, 811, 972, 292]:\n",
        "#     print(tweets_punct[i], end=' => ')\n",
        "#     print(prepare_grammar(tweets_punct[i]))\n",
        "#     print()"
      ],
      "metadata": {
        "id": "rT8nZ-xApv8w"
      },
      "id": "rT8nZ-xApv8w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords, Stemming e Lemmatization\n",
        "\n",
        "Para a análise de sentimentos talvez seja importante manter `stopwords`, como por exemplo:\n",
        "\n",
        "Na sentença `I didn’t like the product (Negative)` a remoção das `stopwords` resultaria em \t`like product (Positive)`."
      ],
      "metadata": {
        "id": "c1Ab6d4UjqeF"
      },
      "id": "c1Ab6d4UjqeF"
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "\n",
        "# #loading the english language small model of spacy\n",
        "# en = spacy.load('en_core_web_sm')\n",
        "# sw_spacy = en.Defaults.stop_words\n",
        "\n",
        "# print(sw_spacy)"
      ],
      "metadata": {
        "id": "aLTmpMfKj0Sf"
      },
      "id": "aLTmpMfKj0Sf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_stopwords(tweet: str):\n",
        "#     return ' '.join([word for word in tweet.split() if word.lower() not in sw_spacy])\n",
        "\n",
        "# tweets_clean = [remove_stopwords(tweet) for tweet in tweets_clean]\n",
        "# tweets_clean"
      ],
      "metadata": {
        "id": "YDqkUfoMeNLK"
      },
      "id": "YDqkUfoMeNLK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora quanto ao processo de `stemming` geralmente pode resultar em palavras inexistentes na gramática e por isso não irei utilizar."
      ],
      "metadata": {
        "id": "wAGvXBm_fJ0z"
      },
      "id": "wAGvXBm_fJ0z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, irei aplicar `lemmatization` reduzindo as palavras a um radical existe de modo que o vocabulário final será reduzido também."
      ],
      "metadata": {
        "id": "Bi6sKn8IfSfU"
      },
      "id": "Bi6sKn8IfSfU"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "UsUNAnYFfcdc"
      },
      "id": "UsUNAnYFfcdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tweet_lemmatization(tweet: str):\n",
        "    doc = nlp(tweet)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "tweets_clean = [tweet_lemmatization(tweet) for tweet in tweets_clean]\n",
        "tweets_clean"
      ],
      "metadata": {
        "id": "hsLyPjXlf_wG"
      },
      "id": "hsLyPjXlf_wG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert(len(tweets_clean) == 10 ** 4)"
      ],
      "metadata": {
        "id": "qzWwKfSThYsI"
      },
      "id": "qzWwKfSThYsI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulário final"
      ],
      "metadata": {
        "id": "-tRVgbGJhlJd"
      },
      "id": "-tRVgbGJhlJd"
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# calculando a fequência de distribuição de tokens\n",
        "tokens = nltk.word_tokenize(' '.join(tweets_clean))\n",
        "freqs = nltk.FreqDist(tokens)\n",
        "\n",
        "print(f'Vocabulário final: {len(freqs.keys())}')\n",
        "total_tokens = sum([freqs[k] for k in freqs])\n",
        "print(f'Total de tokens: {total_tokens}')\n",
        "print(f'Média de tokens por tweet: {freqtot / len(tweets_clean):.2f}')"
      ],
      "metadata": {
        "id": "iMmYYwz_hmkU"
      },
      "id": "iMmYYwz_hmkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparando como o valores iniciais:\n",
        "\n",
        "- Vocabulário: ${V}_{i} = 17.638$ e ${V}_{f} = 10.186$ uma redução de $42.25 \\%$.\n",
        "- Total Tokens: ${TT}_{i} = 104.006$ e ${TT}_{f} = 101.688$ uma redução de $2.23 \\%$"
      ],
      "metadata": {
        "id": "gp-zpkLrizjj"
      },
      "id": "gp-zpkLrizjj"
    },
    {
      "cell_type": "code",
      "source": [
        "# ordenando decrescente por frequencia\n",
        "freqs_sorted = sorted(freqs, key=freqs.get, reverse=True)\n",
        "\n",
        "print('Lista das 20 palavras mais frequentes')\n",
        "for freq in freqs_sorted[:20]:\n",
        "    print(f\"{freq}\\t\\t{freqs[freq]}\")\n"
      ],
      "metadata": {
        "id": "a16b9Bl-5Ecl"
      },
      "id": "a16b9Bl-5Ecl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Top 20 palavras mais frequentes no corpus')\n",
        "sns.barplot(y=[k for k in freqs_sorted[:20]], x=[freqs[k] for k in freqs_sorted[:20]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xY9JGal45t0"
      },
      "id": "6xY9JGal45t0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_sorted = sorted(freqs.keys(), key=len, reverse=True)\n",
        "\n",
        "print('Lista das 20 palavras mais longas')\n",
        "for word in len_sorted[:20]:\n",
        "    print(' ', len(word), '\\t', word)"
      ],
      "metadata": {
        "id": "beFnSaxP58mb"
      },
      "id": "beFnSaxP58mb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Top 20 palavras mais longas')\n",
        "sns.barplot(y=[k for k in len_sorted[:20]], x=[len(k) for k in len_sorted[:20]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "25Uffwdu6KJu"
      },
      "id": "25Uffwdu6KJu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model for Sentiment Analysis (Pipeline)"
      ],
      "metadata": {
        "id": "1XIdOb0W6NU3"
      },
      "id": "1XIdOb0W6NU3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install xformers"
      ],
      "metadata": {
        "id": "7JaIqbL_WB5c"
      },
      "id": "7JaIqbL_WB5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "gsgI52w5j9oS"
      },
      "id": "gsgI52w5j9oS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# set gpu\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "sentiment_pipeline = pipeline(\"text-classification\", device=device)"
      ],
      "metadata": {
        "id": "GAk27etsOiqT"
      },
      "id": "GAk27etsOiqT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def sentiment_score(model_name: str, y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=['POSITIVE', 'NEGATIVE']))\n",
        "\n",
        "    plt.title(f'BERT Confusion Matrix = {model_name.upper()}')\n",
        "    ax=sns.heatmap(cm, annot=True, fmt='d')\n",
        "    ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
        "    plt.plot()\n",
        "\n",
        "    return cm"
      ],
      "metadata": {
        "id": "Dbk9GE8lZEtu"
      },
      "id": "Dbk9GE8lZEtu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_scores(model: str, y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    acc = (cm[0][0] + cm[1][1]) / cm.sum()\n",
        "    err = (cm[0][1] + cm[1][0]) / cm.sum()\n",
        "    n_prec = cm[0][0] / (cm[0][0] + cm[1][0])\n",
        "    p_prec = cm[1][1] / (cm[1][1] + cm[0][1])\n",
        "    n_rec = cm[0][0] / (cm[0][0] + cm[0][1])\n",
        "    p_rec = cm[1][1] / (cm[1][0] + cm[1][1])\n",
        "    return {\n",
        "        'accuracy': [acc],\n",
        "        'error_rate': [err],\n",
        "        'negative_precision': [n_prec],\n",
        "        'positive_precision': [p_prec],\n",
        "        'negative_recall': [n_rec],\n",
        "        'positive_recall': [p_rec]\n",
        "    }"
      ],
      "metadata": {
        "id": "JGexeVQJAisK"
      },
      "id": "JGexeVQJAisK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model_name: str, tweets, y_true):\n",
        "    results = sentiment_pipeline(tweets)\n",
        "    y_pred = [r['label'] for r in results]\n",
        "    cm = sentiment_score(model_name, y_true, y_pred)\n",
        "\n",
        "    model_scores = calculate_scores(model_name, y_true, y_pred)\n",
        "    model_scores['model'] = model_name\n",
        "    return model_scores"
      ],
      "metadata": {
        "id": "UdNZSWAWKXUU"
      },
      "id": "UdNZSWAWKXUU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = ['POSITIVE' for _ in range(5000)] + ['NEGATIVE' for _ in range(5000)]\n",
        "scores = evaluate_model('tweets_base_model', tweets_clean, y_true)\n",
        "scores"
      ],
      "metadata": {
        "id": "KrCTQisMK6ZG"
      },
      "id": "KrCTQisMK6ZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model_scores = pd.DataFrame(scores)\n",
        "df_model_scores"
      ],
      "metadata": {
        "id": "wwG4FM40OKnC"
      },
      "id": "wwG4FM40OKnC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo base usando o `pipeline` de classificação de texto obteve uma acurácia de `0.68`, com precisão de `0.69` para classe `NEGATIVE`. e `0.67` para a classe `POSITIVE`."
      ],
      "metadata": {
        "id": "SH6rPBYKmovB"
      },
      "id": "SH6rPBYKmovB"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}