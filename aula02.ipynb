{"cells":[{"cell_type":"code","execution_count":2,"id":"023b079e","metadata":{"id":"023b079e","executionInfo":{"status":"ok","timestamp":1690940398596,"user_tz":240,"elapsed":4520,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}}},"outputs":[],"source":["import nltk\n","import random"]},{"cell_type":"markdown","id":"ade8dfd0","metadata":{"id":"ade8dfd0"},"source":["# Modelos de Linguagem Probabilísticos\n","\n","## Prof. André Carvalho"]},{"cell_type":"markdown","id":"4f51c000","metadata":{"id":"4f51c000"},"source":["Dada a frase abaixo:\n","\n","> \"Estou molhado porque hoje...\"\n","\n","Qual vocês acham que é a palavra mais provável para continuar esta sentença?"]},{"cell_type":"markdown","id":"ea6799a9","metadata":{"id":"ea6799a9"},"source":["## Modelos de Linguagem Probabilísticos\n","\n","Calculam a probabilidade de uma sentença ocorrer.\n","\n","Útil em diversos domínios:\n"," - Tradução\n"," - Correção de digitação\n"," - Reconhecimento de fala\n"," - etc etc etc"]},{"cell_type":"markdown","id":"33f6a88a","metadata":{"id":"33f6a88a"},"source":["## Modelos de Linguagem Probabilísticos\n","\n","Probabilidade de uma sequência de palavras ocorrer:\n"," - $P(W) = P(w_1,w_2,w_3,w_4,w_5,...,w_N)$\n","\n","Relacionada: Probabilidade da próxima palavra em uma sentença:\n"," - $P(w_5|w_1,w_2,w_3,w_4)$\n","\n","Um modelo que computa estas probabilidades é chamado de _Language Model_.\n","\n","Baseado em frequências de __n-gramas__."]},{"cell_type":"markdown","id":"3009509b","metadata":{"id":"3009509b"},"source":["Como computar a probabilidade da frase:\n","\n","> Um bom dia para ser\n","\n","Ou seja, P(Um,bom,dia,para,ser)\n","\n","Vamos relembrar probabilidade e estatística e usar a regra da cadeia."]},{"cell_type":"markdown","id":"530bf4f9","metadata":{"id":"530bf4f9"},"source":["### Regra da Cadeia\n","\n","Probabilidade condicional:\n"," - $P(B|A) = P(A,B)/P(A)$, que pode ser reescrito como $P(A,B) = P(A)P(B|A)$\n","\n","Para mais variáveis:\n"," - $P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)$\n","\n","Ampliando assim para quaisquer N variáveis.\n"," - $P(w_1,w_2...w_n) = \\prod_i{P(w_i|w_1,w_2...w_{i-1})}$"]},{"cell_type":"markdown","id":"ea1dfe1d","metadata":{"id":"ea1dfe1d"},"source":["$$P(w_1,w_2...w_n) = \\prod_i{P(w_1|w_1,w_2...w_{i-1})}$$\n","\n","$$P(\\text{\"Um bom dia para ser\"})= P(um).P(bom|um).P(dia|um,bom).\n","P(para|um,bom,dia).P(ser|um,bom,dia,para)$$\n","\n","Como estimar estas probabilidades?\n","\n"]},{"cell_type":"markdown","id":"b30c4355","metadata":{"id":"b30c4355"},"source":["Contar e dividir?\n","\n","$P(\"brasileiro\"|\\text{\"Um bom dia para ser\"}) = $\n","\n","$$ \\frac{freq(\"Um,bom,dia,para,ser,brasileiro\")}{freq(\"Um,bom,dia,para,ser\")} $$\n","\n","Vamos ver no google qual a probabilidade?\n","\n","Problema: Muitas possibilidades de frases, com tamanhos radicalmente diferentes."]},{"cell_type":"markdown","id":"9a2366fb","metadata":{"id":"9a2366fb"},"source":["## Suposição Markoviana\n","\n","__A probabilidade de uma palavra depende apenas da palavra anterior.__\n","\n","Afrouxando um pouco, podemos assumir que depende das _n_ palavras anteriores.\n"," - N-Gramas.\n","\n","$P(\"brasileiro\"|\\text{\"Um bom dia para ser\"}) \\approx P(\"brasileiro\"|\"ser\")$\n","\n","ou\n","\n","$P(\"brasileiro\"|\\text{\"Um bom dia para ser\"}) \\approx P(\"brasileiro\"|\\text{\"para ser\"})$"]},{"cell_type":"markdown","id":"9e5ef84e","metadata":{"id":"9e5ef84e"},"source":["## Suposição Markoviana\n","\n","$P(w_1,w_2...w_n) \\approx \\prod_i{P(w_i|w_{i-k}...w_{i-1})}$\n","\n","Ou seja, quanto maior o k, mais aproximamos:\n","\n","$P(w_1|w_1w_2...w_n) \\approx \\prod_i{P(w_i|w_{i-k}...w_{i-1})}$"]},{"cell_type":"markdown","id":"f01038d7","metadata":{"id":"f01038d7"},"source":["# Modelo Unigrama\n","\n","$P(w_1,w_2...w_n) \\approx \\prod_i{P(w_i)}$\n","\n","Vamos ver sentenças geradas automaticamente baseado em Dom Casmurro:"]},{"cell_type":"code","execution_count":null,"id":"f67f6c44","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f67f6c44","executionInfo":{"status":"ok","timestamp":1690914883609,"user_tz":240,"elapsed":635,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"295d9738-aa70-407b-d742-7211a2be8028"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package machado to /root/nltk_data...\n","[nltk_data]   Package machado is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["0.037615565895220256\n"]}],"source":["import string\n","# nltk.download('machado')\n","# nltk.download('punkt')\n","\n","dom_casmurro = nltk.corpus.machado.raw('romance/marm08.txt')\n","tokens = nltk.word_tokenize(dom_casmurro.lower().translate(str.maketrans('','',string.punctuation)))\n","\n","freqs = nltk.FreqDist(tokens)\n","freqtot = sum(freqs.values())\n","\n","probs = {x:freqs[x]/freqtot for x in freqs.keys()}\n","\n","\n","print(probs[\"a\"])"]},{"cell_type":"code","execution_count":null,"id":"8dcec705","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dcec705","executionInfo":{"status":"ok","timestamp":1690914978552,"user_tz":240,"elapsed":472,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"453f615a-b240-4418-b530-42f27570b9e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["outrora,repetiamse,pertencem,souber,apareceme,voltasse,noite,amuado,natureza,bairro,"]}],"source":["import random\n","\n","def unigram():\n","    prob = random.random()\n","    v = 0\n","    for i in probs:\n","        if(v>prob):\n","            return i\n","        v+=probs[i]\n","\n","for i in range(10):\n","    print(unigram(),end=',')"]},{"cell_type":"markdown","id":"029c6763","metadata":{"id":"029c6763"},"source":["## Modelos de Bigrama\n","\n","Probabilidade condicionada na palavra anterior.\n","\n","$P(w_1,w_2...w_n) \\approx \\prod_i{P(w_i|w_{i-1})}$\n","\n","Vamos implementar um modelo de bigramas."]},{"cell_type":"markdown","id":"77ea67ac","metadata":{"id":"77ea67ac"},"source":["## Estimando probabilidades de N-Gramas\n","\n","Estimando a máxima verossimilhança de bigramas:\n","\n","$$P(w_i|w_{i-1}) = \\frac{f(w_{i-1},w_{i})}{f(w_{i-1})} $$"]},{"cell_type":"markdown","id":"515a1cb0","metadata":{"id":"515a1cb0"},"source":["Exemplo:\n","\n","Num corpus com 3 sentenças vamos calcular as probabilidades:\n","\n","> [s] Eu sou André [/s]\n","\n","> [s] André, sou eu [/s]\n","\n","> [s] Eu sou manauara [/s]"]},{"cell_type":"code","execution_count":null,"id":"36cbd879","metadata":{"id":"36cbd879"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"94d88ea9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94d88ea9","executionInfo":{"status":"ok","timestamp":1690915821327,"user_tz":240,"elapsed":617,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"9d93f740-101e-4649-e0f0-94fc395c95b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.8\n"]}],"source":["list(nltk.bigrams(tokens))\n","big = nltk.bigrams(tokens)\n","\n","freqbi = nltk.FreqDist(big)\n","freqtotbi = sum(freqbi.values())\n","\n","probi = {x:freqbi[x]/freqs[x[0]] for x in freqbi.keys()}\n","\n","\n","print(probi[(\"dom\",\"casmurro\")])"]},{"cell_type":"code","execution_count":null,"id":"a70411da","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a70411da","executionInfo":{"status":"ok","timestamp":1690916365000,"user_tz":240,"elapsed":679,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"c5933fcb-e310-461c-9b4c-233005b386ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["deixara como pessoas valem só dessas caprichosas me consolassem da "]}],"source":["#vamos tentar fazer agora?\n","#modifique para ser bigrama\n","\n","def bigram(inverted, anterior):\n","    prob = random.random()\n","    v = 0\n","    for i in inverted[anterior]:\n","        if(v > prob):\n","            return i\n","        v += probs[i]\n","    return i\n","\n","inverted = {}\n","for i in probi:\n","  if i[0] not in inverted:\n","    inverted[i[0]] = {i[1]:probi[i]}\n","  else:\n","    inverted[i[0]][i[1]] = probi[i]\n","\n","# print(inverted[\"dom\"])\n","\n","# for i in range(10):\n","#    print(bigram(inverted, 'dom'), end=',')\n","\n","ant = 'eu'\n","for i in range(10):\n","    p = bigram(inverted, ant)\n","    print(p, end=' ')\n","    ant = p"]},{"cell_type":"code","source":["inverted[\"dom\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewyQm2hVeKIt","executionInfo":{"status":"ok","timestamp":1690916403293,"user_tz":240,"elapsed":613,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"6e136445-ed2e-4278-e04f-9fa052e87a53"},"id":"ewyQm2hVeKIt","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'casmurro': 0.8, 'veio': 0.1, 'de': 0.1}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["def bigram2(inverted, anterior):\n","    return max(inverted[anterior], key=inverted[anterior].get)\n","\n","ant = 'eu'\n","for i in range(10):\n","    p = bigram2(inverted, ant)\n","    print(p, end=' ')\n","    ant = p"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaP5OUc9eVU-","executionInfo":{"status":"ok","timestamp":1690916800626,"user_tz":240,"elapsed":510,"user":{"displayName":"Marcos Avner Pimenta de Lima","userId":"01744861731105057305"}},"outputId":"50f72413-e05b-4359-d650-8918d3af518a"},"id":"WaP5OUc9eVU-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["não me lembra que não me lembra que não me "]}]},{"cell_type":"markdown","id":"8f7273b1","metadata":{"id":"8f7273b1"},"source":["Podemos ampliar os modelos para tri, 4, 5-gramas ou até mais.\n","\n","Um modelo limitado de linguagem.\n"," - Dependências de longa distância.\n","\n","Mas pra muitas tarefas modelos de N-gramas resolvem bem."]},{"cell_type":"markdown","id":"485b9302","metadata":{"id":"485b9302"},"source":["# Estimando probabilidade de sentenças:\n","\n","\n","P([s] Um bom dia para ser brasileiro [/s]) =\n","\n","P(Um|[s]) x P(Bom|Um) x P(dia|bom) x\n","\n","P(para|dia) x P(ser|para) x P(Brasileiro|para)\n","\n","\n","Probabilidades: sempre em espaço logarítmico:\n"," - Diminuir underflow\n"," - Somar é mais rápido que multiplicar\n","\n","$log(p_1.p_2.p_3) = log(p_1)+log(p_2)+log(p_3)$"]},{"cell_type":"markdown","id":"91893711","metadata":{"id":"91893711"},"source":["# Ferramentas para modelos de linguagem\n","\n","SRILM\n","\n","http://www.speech.sri.com/projects/srilm/\n","\n","\n","KenLM\n","\n","https://kheafield.com/code/kenlm/\n","\n"]},{"cell_type":"markdown","id":"04a9ccbb","metadata":{"id":"04a9ccbb"},"source":["### Google N-grams\n","\n","Em 2006 o google lançou um dataset com frequencias de N-gramas de 1-5, com 13.588.391 palavras únicas (todas com frequência acima de 200)\n","\n","https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html\n","\n","https://books.google.com/ngrams"]},{"cell_type":"markdown","id":"73e877b7","metadata":{"id":"73e877b7"},"source":["# Avaliando Modelos de Linguagem\n","\n","O modo ideal de avaliar um modelo de linguagem seria _avaliação extrínsica_.\n"," - Bota os 2 modelos para rodar no mundo real e ve qual se sai melhor.\n"," - Impraticável em muitos casos, precisa de tempo e pessoas.\n","\n","O jeito é fazer avaliação intrínsica:\n"," - Usar um conjunto de teste com holdout.\n"," - Escolher alguma métrica como _perplexidade_.\n"," - Problema: Se o teste não for parecido com o treino."]},{"cell_type":"markdown","id":"6ffb69dc","metadata":{"id":"6ffb69dc"},"source":["## Perplexidade\n","\n","A perplexidade de um modelo de linguagem em um conjunto de teste é _o inverso_ da probabilidade do conjunto de teste ser gerado pelo modelo, normalizado pelo tamanho do vocabulário.\n","\n","Dado um teste $w_1w_2w_3...w_N$:\n","\n","$$PP(W) = P(w_1,w_2,...w_N)^{\\frac{-1}{N}} = $$\n","\n","$$ \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_{i-k}...w_{i-1})}}$$\n","\n","Inversamente correlacionada com a probabilidade (maior é pior)."]},{"cell_type":"markdown","id":"478ef8f6","metadata":{"id":"478ef8f6"},"source":["## Entendendo perplexidade\n","\n","Perplexidade pode ser vista como uma árvore de escolhas ponderadas.\n","\n","Quão dificil é uma linguagem teste com os digitos de 0-9 ocorrendo com a mesma frequência?\n","\n","$$P(W) = ( \\frac{1}{10}^N)^{-\\frac{1}{N}} $$\n","\n","$$ \\frac{1}{10}^{-1} = 10$$\n","\n","Um modelo treinado com um treino onde, por exemplo, o dígito 0 tenha frequência muito maior, vai ter uma perplexidade mais alta no teste, pois o modelo vai encontrar menos 0 do que imagina."]},{"cell_type":"markdown","id":"1bd504a1","metadata":{"id":"1bd504a1"},"source":["### Gerando sentenças\n","\n","Escolha um bigrama aleatório ([s],w) que comece com um token [s] de início de sentença de acordo com as probabilidades.\n","\n","Escolha bigramas (w,x) também pelas probabilidades.\n","\n","Repita até encontrar um bigrama de fim de frase (x,[/s])"]},{"cell_type":"code","execution_count":null,"id":"b3de19da","metadata":{"id":"b3de19da"},"outputs":[],"source":["# Preparar o machado de assis aqui"]},{"cell_type":"markdown","id":"4df2fa80","metadata":{"id":"4df2fa80"},"source":["### Cuidado com overfit\n","\n","Modelos de N-Gramas funcionam bem desde que o teste pareça com o treino.\n"," - É comum que não pareça.\n"," - Aspectos temporais por exemplo.\n","\n","Os modelos tem que generalizar.\n"," - Mas o que fazer com as frequências zero?\n"," - Coisas que não estão no treino mas estão no teste."]},{"cell_type":"markdown","id":"c0e28682","metadata":{"id":"c0e28682"},"source":["### Palavras desconhecidas\n","\n","Uma forma de tratar este problema, quando trabalhamos com vocabulário fixo, é usar um token especial \\<UNK\\>.\n"," - O vocabulario pode ser podado com uma frequência fixa, ou as top-K mais frequentes.\n"," - Todas palavras podadas nesta fase no treino são substituídas por \\<UNK\\>\n"," - O modelo aprende uma distribuição de probabilidades considerando as desconhecidas.\n"," - Assim ele pode prever estas palavras.\n","\n","Cuidado! Um vocabulário muito curto pode fazer com que a perplexidade __caia__. Por que?"]},{"cell_type":"markdown","id":"38bbd831","metadata":{"id":"38bbd831"},"source":["Bigramas com frequência zero no treino vão ter probabilidade zero.\n","\n","O que vai fazer a probabilidade do teste ser zero também.\n","\n","Isso vai fazer a perplexidade literalmente tender ao infinito (já que vai ter uma divisão por zero)."]},{"cell_type":"markdown","id":"6973248d","metadata":{"id":"6973248d"},"source":["## Smoothing\n","\n","Uma saída para isto é suavizar as probabilidades, adicionando valores para todas probabilidades zero terem uma chance mínima.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","id":"b44a8e1b","metadata":{"id":"b44a8e1b"},"source":["# Smoothing de Laplace (ou soma-um)\n","\n","Imagine que cada palavra apareceu uma vez a mais no treino (ou seja, todas as probabilidades tem frequência mínima 1.\n","\n","$$ P_l(w_i|w_{i-1}) = \\frac{f(w_i|w_{i-1})+1}{f(w_{i-1})+V} $$"]},{"cell_type":"markdown","id":"52bf228b","metadata":{"id":"52bf228b"},"source":["Na prática, é como se um pouco da frequencia dos bigramas fossem redistribuídos para os zeros:\n","\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","id":"ad7f6fcf","metadata":{"id":"ad7f6fcf"},"source":["Laplace não é normalmente usado para modelos de N-gramas.\n"," - Ele mexe demais nas probabilidades de ordem de palavras.\n","\n","Bem comum em outras tasks, como classificação, onde não existem tantos zeros."]},{"cell_type":"markdown","id":"d9e6fcac","metadata":{"id":"d9e6fcac"},"source":["## Backoff e Interpolação\n","\n","Existem casos em que contexto demais pode atrapalhar.\n"," - Especialmente em contextos que não são tão comuns.\n","\n","### Backoff\n"," - Use trigramas se forem bons pra situação.\n"," - Senão bigramas, senão unigramas.\n","\n","### Interpolação\n"," - misturar unigramas, bigramas e trigramas"]},{"cell_type":"markdown","id":"e466a1d6","metadata":{"id":"e466a1d6"},"source":["## Interpolação Linear\n","\n","#### Interpolação simples\n","\n","$$ \\begin{aligned}\\hat{P}(w_n|w_{n-2}w_{n-1}) =  \\lambda_1 P(w_n|w_{n-2}w_{n-1}) \\\\\n"," + \\lambda_2 P(w_n|w_{n-1}) \\\\\n"," + \\lambda_3 P(w_n) \\end{aligned}$$\n","\n","#### Interpolação com contexto\n","\n","$$ \\begin{aligned}\\hat{P}(w_n|w_{n-2}w_{n-1}) =  \\lambda_1(w_{n-2}^{n-1}) P(w_n|w_{n-2}w_{n-1}) \\\\\n"," + \\lambda_2(w_{n-2}^{n-1}) P(w_n|w_{n-1}) \\\\\n"," + \\lambda_3(w_{n-2}^{n-1}) P(w_n) \\end{aligned}$$\n"]},{"cell_type":"markdown","id":"9231c68f","metadata":{"id":"9231c68f"},"source":["## Como escolher os Lambdas\n","\n","Hold-out com __validação__.\n","\n","Escolha os $\\lambda$s que maximizem a probabilidade da validação (minimizando a perplexidade).\n"," - Fixar as probabilidades de N-gramas do modelo geradas no treino.\n"," - Buscar pelos $\\lambda$s usando algum algoritmo de maximização de verossimilhança, como o EM.\n"]},{"cell_type":"markdown","id":"9054298f","metadata":{"id":"9054298f"},"source":["## Otimizações para corpus grandes\n","\n","Poda de vocabulário\n"," - Só guardar informações sobre n-gramas com frequência maior que um limiar.\n"," - Poda baseada em entropia.\n","\n","Eficiência\n"," - Estruturas de dados como tries.\n"," - Filtros de Bloom: aproximações de modelos de linguagem\n"," - Armazenar palavras como índices ao invés de strings.\n","     - Comprimindo, p. exemplo com árvores de Huffman.\n"," - Quantização: Armazenar as probabilidades com menos bits ao invés de um float."]},{"cell_type":"code","execution_count":null,"id":"66ec1889","metadata":{"id":"66ec1889"},"outputs":[],"source":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}