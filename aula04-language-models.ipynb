{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d89d773",
   "metadata": {
    "id": "0d89d773"
   },
   "source": [
    "\n",
    " <img src=\"https://drive.google.com/uc?id=18x0Fa9XWHlnH5OWkZ-UMrJVQCdsEmYQw\" width=300/>\n",
    " \n",
    "## PPGINF 528 TSRI / ICC220 TSBD - NLP [Jul/2023]\n",
    "## Processamento de Linguagem Natural\n",
    "#### Professores André Carvalho e Altigran da Silva\n",
    "### Aula 4 - Representação de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c5723",
   "metadata": {
    "id": "3e3c5723"
   },
   "source": [
    "## Tratamento de Documentos Textuais\n",
    "--- \n",
    "\n",
    "- Diversas aplicações importantes envolvem tratamento de documentos textuais. \n",
    "- Esta aplicações podem utilizar várias técnicas computacionais, entre elas:\n",
    "  - Mineração de texto: cluster de documentos, extração de tópicos \n",
    "  - Categorização: atribuir documentos a categorias em taxonomias\n",
    "  - Extração de entidades: extração de nomes, endereços, dados de produtos etc.\n",
    "  - Análise de sentimento: avaliar se um documento/ é positiva, negativa ou neutro quanto a um tópico\n",
    "  - Linguística: extração de semântica, propósito, etc.\n",
    "- Técnicas implementadas por algoritmos de ML e/ou RI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b5241",
   "metadata": {
    "id": "0b0b5241"
   },
   "source": [
    "## Tratamento de Documentos Textuais\n",
    "---\n",
    "\n",
    "- Vários destes algoritmos manipulam __vetores de características__ (_feature vectores_) \n",
    "\n",
    "- No caso de documentos textuais é preciso representar adequadamente os documentos utilizando estes vetores.\n",
    "\n",
    "- Exemplos de documentos: páginas Web, avaliações de usuários, artigos científicos, posts de usuários em redes sociais, documentos legais, médicos, financeiros, administrativos, ...\n",
    "\n",
    "- Embora muitos diversos entre si, existem formas de representação comuns que funcionam bem para várias aplicações.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c88c5a",
   "metadata": {
    "id": "18c88c5a"
   },
   "source": [
    "## Representação de Documentos\n",
    "---\n",
    "\n",
    "- Vetor  $\\color{blue}{d = \\langle w_1,w_2,\\ldots,w_n \\rangle}$ representa o doc no espaço $\\color{blue}{n}$-dimensional\n",
    "\n",
    "- Alternativas\n",
    "  -  Ocorrência: $\\color{blue}{w_i}=1$ se ocorre no doc ou $\\color{blue}{w_i}=0$ se não ocorre\n",
    "  - _Bag-of-Words_: $\\color{blue}{w_i}$ é a frequência do termo no documento\n",
    "  - _n-gramas_ : $\\color{blue}{w_i}$ é a frequência do n-grama no documento\n",
    "  - _TF-IDF_:  $\\color{blue}{w_i}$ calculado usando pesos TF-IDF\n",
    "  - _Embdeddingd_:  $\\color{blue}{w_i}$ reflete o contexto linguístico da palavra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011be92",
   "metadata": {
    "id": "1011be92"
   },
   "source": [
    "## Modelo Vetorial\n",
    "---\n",
    "- _Vector Space Moldel_ [[Salton et al.,1975]](https://dl.acm.org/doi/10.1145/361219.361220)\n",
    "- Documentos de texto representados por vetores \n",
    "  - $\\color{blue}{d = \\langle w_1, w_2,\\ldots, w_n\\rangle}$\n",
    "  - $\\color{blue}{w_i \\neq 0}$ se a palavra $\\color{blue}{x_i}$ ocorre no documento\n",
    "  - $\\color{blue}{w_i = 0}$ se a palavra $\\color{blue}{x_i}$ não ocorre no documento\n",
    " -  $\\color{blue}{\\{ x_1, x_2,\\ldots, x_n\\}}$ = __vocabulário__ : conjunto de todas as palavras que ocorrem na coleção de documentos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42e9180",
   "metadata": {
    "id": "c42e9180"
   },
   "source": [
    "## Modelo Vetorial\n",
    "--- \n",
    "\n",
    "  - A cada palvra ou _termo_ $\\color{blue}{x_i}$ é associado um vetor unário \n",
    "    - Assume-se que os vetores de termos são ortogonais\n",
    "    - Isso implica em que assumir que os termos ocorrem independentemente\n",
    "  - Os $\\color{blue}{t}$ vetores unários formam a base ortonormal de um espaço $\\color{blue}{t}$-dimensional\n",
    "  \n",
    "  - Neste espaço, documentos são representados como vetores ponderados.\n",
    " \n",
    "<center>\n",
    "$\\color{blue}{\\overrightarrow{d_{j}}=\\left(w_{1 j}, w_{2 j}, \\ldots, w_{tj}\\right), k_{i} \\in d_j w_{ij}>0}$\n",
    "</center>\n",
    "\n",
    "  - No contexto de busca de documentos, _consultas_ também são representados assim\n",
    " \n",
    "<center>\n",
    "$\\color{blue}{\\overrightarrow{q}=\\left(w_{1 q}, w_{2 q}, \\ldots, w_{tq}\\right), k_{i} \\in q, w_{iq}>0}$\n",
    " </center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8e86d",
   "metadata": {
    "id": "cba8e86d"
   },
   "source": [
    "## Modelo Vetorial - Pesos Baseados em Ocorrência\n",
    "---\n",
    "- Projeções com tamanho 0 ou 1: Peso 1 se o termo ocorre no doc, ou 0 se não ocorre\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "   <td>  </td> <td>  </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    " <td>\n",
    "    <center> <img src=\"https://drive.google.com/uc?id=1FTgozyIe09-88Anhu8M5PQmy7d8OzELg\" width=300/> </center>\n",
    "</td>\n",
    "   <td>\n",
    "       \n",
    "|       | hard  | drive | test  | $$Q\\cdot{D_x}$$   | $$||D_x||$$     | $$\\cos(\\theta)$$ | $$\\theta$$ (graus) |\n",
    "| :---  | :---: | :---: | :---: | :---:              | :---: | :---:      | :---:              |\n",
    "| $$Q$$     |     1 | 1     | 1     |                    | 1.732 |            |                    |\n",
    "|$$D_1$$| 1 | 0 | 0 | 1 | 1.000 | 0.577 | 54.736 |\n",
    "|$$D_2$$| 1 | 1 | 0 | 2 | 1.414 | 0.816 | 35.264 |\n",
    "|$$D_3$$| 0 | 1 | 1 | 2 | 1.414 | 0.816 | 35.264 |\n",
    "|$$D_4$$| 1 | 1 | 1 | 3 | 1.732 | 1.000 | 0 |\n",
    "|$$D_5$$| 1 | 0 | 1 | 2 | 1.414 | 0.816 | 35.264 |\n",
    "       \n",
    "   <td>\n",
    "</tr>\n",
    "</table>       \n",
    "\n",
    "<table>\n",
    "<tr> <td> $Q$ = <b>hard drive test</b> <td> </tr>\n",
    "<tr> <td> $D_1$ = \"...it's <b>hard</b> to determine...\" <td> </tr>\n",
    "    <tr> <td> $D_2$  = \"...this <b>hard drive</b> has 100GB of memory...make sure the <b>drive</b> is fully installed...} <td> </tr>\n",
    "<tr> <td> $D_3$  = \"...before I bought my new car, I took it out for a <b>test drive</b>...\"  <td> </tr>\n",
    "<tr> <td> $D_4$  = \"...as part of the factory acceptance, every unit gets a <b>hard drive test</b>...\"  <td> </tr>\n",
    "<tr> <td> $D_5$  = \"...I think I failed, that was a <b>hard test</b>...a standardized <b>test</b> is design to be <b>hard</b> for...\" <td> </tr>\n",
    "<table>\n",
    "    \n",
    "    \n",
    "<font size=\"2\">[Adaptado de Simple Data Bining Blog](http://simpledatamining.blogspot.com/2015/03/simple-text-retrieval-vector-space.html)</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187b07d",
   "metadata": {
    "id": "4187b07d"
   },
   "source": [
    "## Modelo Vetorial - Pesos Baseados em Frequência\n",
    "---\n",
    "- Assume projeções/pesos baseados na frequência dos termos dos documentos\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "   <td>  </td> <td>  </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    " <td>\n",
    "    <center> <img src=\"https://drive.google.com/uc?id=1FT5fNwHUQBj0lcX6WiMkbBTZRuGcluMq\" width=300/> </center>\n",
    "</td>\n",
    "   <td>\n",
    "       \n",
    "|       | hard  | drive | test  | $$Q\\cdot{D_x}$$   | $$||D_x||$$     | $$\\cos(\\theta)$$ | $$\\theta$$ (graus) |\n",
    "| :---  | :---: | :---: | :---: | :---:              | :---: | :---:      | :---:              |\n",
    "| Q     |     1 | 1     | 1     |                    | 1.732 |            |                    |\n",
    "|$$D_1$$| 1 | 0 | 0 | 1 | 1.000 | 0.577 | 54.736 |\n",
    "|$$D_2$$| 1 | 2 | 0 | 3 | 2.236 | 0.775 | 39.232 |\n",
    "|$$D_3$$| 0 | 1 | 1 | 2 | 1.414 | 0.816 | 35.264 |\n",
    "|$$D_4$$| 1 | 1 | 1 | 3 | 1.732 | 1.000 | 0 |\n",
    "|$$D_5$$| 2 | 0 | 2 | 4 | 2.828 | 0.816 | 35.264 |\n",
    "       \n",
    "   <td>\n",
    "</tr>\n",
    "</table>       \n",
    "\n",
    "<table>\n",
    "<tr> <td> $Q$ = <b>hard drive test</b> <td> </tr>\n",
    "<tr> <td> $D_1$ = \"...it's <b>hard</b> to determine...\" <td> </tr>\n",
    "    <tr> <td> $D_2$  = \"...this <b>hard drive</b> has 100GB of memory...make sure the <b>drive</b> is fully installed...} <td> </tr>\n",
    "<tr> <td> $D_3$  = \"...before I bought my new car, I took it out for a <b>test drive</b>...\"  <td> </tr>\n",
    "<tr> <td> $D_4$  = \"...as part of the factory acceptance, every unit gets a <b>hard drive test</b>...\"  <td> </tr>\n",
    "<tr> <td> $D_5$  = \"...I think I failed, that was a <b>hard test</b>...a standardized <b>test</b> is design to be <b>hard</b> for...\" <td> </tr>\n",
    "<table>\n",
    "    \n",
    "    \n",
    "<font size=\"2\">[Adaptado de Simple Data Bining Blog](http://simpledatamining.blogspot.com/2015/03/simple-text-retrieval-vector-space.html)</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74cdb9",
   "metadata": {
    "id": "db74cdb9"
   },
   "source": [
    "## Modelo Vetorial - Caso Geral\n",
    "---\n",
    "\n",
    "- Coleção de Documentos: $\\color{blue}{d_1,d_2,\\ldots,d_j,\\ldots,d_{n-1},d_{n}}$\n",
    "\n",
    "- $\\color{blue}{w_{i,j}}$: tamanho da projeção do termo $\\color{blue}{x_{i}}$ no eixo $\\color{blue}{j}$ ou “peso” de $\\color{blue}{x_{i}}$ no documento $\\color{blue}{d_j}$\n",
    "\n",
    "- $\\color{blue}{\\overrightarrow{d_{j}}=\\left(w_{1 j}, w_{2 j}, \\ldots, w_{tj}\\right), k_{i} \\in d_j \\mbox{ e } w_{ij}>0}$: Vetor que representa  $\\color{blue}{d_j}$ no espaço multi-dimensional de termos.\n",
    "\n",
    "\n",
    "- _Bag-of-Words_ é um caso particular do modelo vetorial onde $\\color{blue}{w_{i,j}}$  é a frequência do termo no documento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2a60b",
   "metadata": {
    "id": "fad2a60b"
   },
   "source": [
    "## Pesos - Intuição\n",
    "---\n",
    "\n",
    "- Sejam\n",
    "  - $\\color{blue}{C}$ uma coleção de documentos (corpus) \n",
    "  - $\\color{blue}{A \\subseteq C}$ um conjunto de documentos considerados relevantes de acordo com a especificação de uma consulta do usuário\n",
    "  \n",
    "- Determinar\n",
    "  - O que caracteriza os docs em a $\\color{blue}{A}$ ⇨ Similaridade Intra-Cluster (SIC)\n",
    "  -  O que caracteriza os docs fora de  $\\color{blue}{A}$ ⇨ Disimilaridade Inter-Cluster (DIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f533476",
   "metadata": {
    "id": "0f533476"
   },
   "source": [
    "## Pesos - Intuição\n",
    "---\n",
    "\n",
    "- Similaridade Intra-Cluster: medida pela frequência bruta de $\\color{blue}{x_i}$ em $\\color{blue}{d_j}$\n",
    "  - Fator $\\color{blue}{\\textit{tf}(i,j)}$: fornece a medida de quão bem o termo descreve o documento.\n",
    "  \n",
    "- A disimilaridade Inter-Cluster mede o inverso da frequência do termo $\\color{blue}{x_i}$ na coleção \n",
    "   - Fator $\\color{blue}{\\textit{idf}(i)}$: termos que aparecem em muitos documentos não são uteis para distinguir a sua relevância.\n",
    "   \n",
    "- Devemos tentar _balancear_ estes dois efeitos\n",
    "  - $\\color{blue}{w_{i,j}=\\textit{tf}(i,j)\\times\\textit{idf}(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29cebf",
   "metadata": {
    "id": "4d29cebf"
   },
   "source": [
    "## Frequência do Termo - tf\n",
    "---\n",
    "\n",
    "-  _Term Frequency_  \n",
    "-  A frequência normalizada do termo $\\color{blue}{x_i}$ no doc $\\color{blue}{d_j}$ é:\n",
    "   - $\\color{blue}{\\text{tf}(i,j)=\\frac{\\text{freq}_{i,j}}{\\max_l\\text{freq}_{l,j}}}$\n",
    "   \n",
    "- $\\color{blue}{\\text{freq}_{i,j}}$= freq. bruta do termo $\\color{blue}{x_i}$ no doc $\\color{blue}{d_j}$  = num. de vezes em que $\\color{blue}{x_i}$  aparece em $\\color{blue}{d_j}$\n",
    "\n",
    "- $\\color{blue}{\\max_l\\text{freq}_{l,j}}$ = freq. bruta do termo $\\color{blue}{x_l}$ mais frequente em $\\color{blue}{d_j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59dc60",
   "metadata": {
    "id": "fa59dc60"
   },
   "source": [
    "## Frequência Inversa em Documentos - idf\n",
    "---\n",
    "\n",
    "- _Inverse Document Frequency_ \n",
    "\n",
    "- $\\color{blue}{\\text{idf}(i)=\\log \\left(\\frac{N}{n_i}\\right)}$\n",
    "  - $\\color{blue}{N}$ = nr. total de docs. na coleção\n",
    "  - $\\color{blue}{n_i}$ = nr. de docs. em que o termo  $\\color{blue}{x_i}$ ocorre\n",
    "\n",
    "- Tenta expressar a importância de um termo dentro de uma coleção.\n",
    "\n",
    "- Quanto mais raro o termo, maior seu idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d08cb",
   "metadata": {
    "id": "a60d08cb"
   },
   "source": [
    "## Pesos: Esquema tf-idf\n",
    "---\n",
    "\n",
    "- Assim temos:\n",
    "\n",
    "    - $\\color{blue}{w_{i,j}=\\textit{tf}(i,j)\\times\\textit{idf}(i)}$\n",
    "\n",
    "    - $\\color{blue}{w_{i,j}=\\textit{tf}(i,j)\\times\\log \\left(\\frac{N}{n_i}\\right)}$\n",
    "    \n",
    "- O log tem a função de tornar as duas grandezas comparáveis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61b00f",
   "metadata": {
    "id": "1a61b00f"
   },
   "source": [
    "## Exemplo\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "   <td> \n",
    "       \n",
    "|     Docs    |     Termos     |\n",
    "|:-:|:-:|\n",
    "|     $$\\color{blue}{d_1}$$      |     A A A B    |\n",
    "|     $$\\color{blue}{d_2}$$     |     A A C      |\n",
    "|     $$\\color{blue}{d_3}$$     |     A A        |\n",
    "|     $$\\color{blue}{d_4}$$     |     B B        |\n",
    "   \n",
    "       \n",
    "   <td>  \n",
    "        $$\\color{blue}{\\text{idf}(A)=\\log \\left(\\frac{4}{3}\\right)=0,28}$$  \n",
    "\n",
    "$$\\color{blue}{\\text{idf}(B)=\\log \\left(\\frac{4}{2}\\right)=0,69}$$\n",
    "\n",
    "$$\\color{blue}{w\\left(d_1, A\\right)=i d f(A) \\times t f\\left(d_1, A\\right)-0,28 \\times 3=0,84}$$ \n",
    "\n",
    "$$\\color{blue}{w\\left(d_1, B\\right)=i d f(B) \\times t f\\left(d_1, B\\right)-0,69 \\times 1=0,69}$$\n",
    "\n",
    "$$\\color{blue}{w\\left(d_1, C\\right)=i d f(C) \\times t f\\left(d_1, C\\right)-1,28 \\times 0=0}$$\n",
    "\n",
    "$$\\color{blue}{\\overrightarrow{d_1}=(0,84 ; 0,69 ; 0)}$$\n",
    "   </td>\n",
    "  </tr>\n",
    "<table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec95c830",
   "metadata": {
    "id": "ec95c830"
   },
   "source": [
    "## Modelo Vetorial - Similaridade \n",
    "--- \n",
    "\n",
    "- _Similariade_ entre documentos: o quão próximo os vetores estão no espaço de termos?\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1FN9czlmZEkp4IwWgDWUCGgzOZrswwwHA\" width=600/> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edb49f",
   "metadata": {
    "id": "64edb49f"
   },
   "source": [
    "## Modelo Vetorial - Medida do Cosseno\n",
    "---\n",
    "\n",
    "- _Similariade_ entre documentos: o quão próximo os vetores estão no espaço de termos?\n",
    "\n",
    "   - $\\color{blue}{\\text{sim}\\left(d_j, d_k\\right)=\\cos(\\theta)}$\n",
    "\n",
    "   - $\\color{blue}{\\text{sim}\\left(d_j, d_k\\right)=\\frac{\\vec{d_j} \\bullet \\vec{d_k}}{|\\vec{d_j}| \\times|\\vec{d_k}|}=\\frac{\\sum_{i=1}^t w_{i,j} \\times w_{i,k}}{\\sqrt{\\sum_{i=1}^t w_{i,j}^2} \\times \\sqrt{\\sum_{i=1}^t w_{i,k}^2}}}$\n",
    "\n",
    "- Para $\\color{blue}{w_{i,j} \\geq 0}$  e  $\\color{blue}{w_{i,k} \\geq 0}$,  $\\color{blue}{\\text{sim}\\left(d_j, k\\right) \\in [0,1]}$\n",
    "\n",
    "\n",
    "- No contexto de tarefas de busca,  $\\color{blue}{d_k}$ é uma consulta $\\color{blue}{q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e3aa4",
   "metadata": {
    "id": "051e3aa4"
   },
   "source": [
    "## Pesos tf-idf em Consultas\n",
    "---\n",
    "\n",
    "- No caso das consultas um outro esquema de ponderação pode ser usado:\n",
    "\n",
    "   - $\\color{blue}{w(i,q)=\\left(0,5+\\frac{0,5 \\text{freq}_{i, q}}{\\max _l \\text { freq }_{l, q}}\\right) \\times \\log \\left(\\frac{N}{n_i}\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e9218",
   "metadata": {
    "id": "e40e9218"
   },
   "source": [
    "## Hipótese Distribucional e Semântica Vetorial\n",
    "--- \n",
    "\n",
    "- Hipótese Distribucional (HD): Palavras com significados similares ocorrem em contextos similares.\n",
    "  - Formulada nos anos 1950 por vários linguistas \n",
    "\n",
    "- Semântica Vetorial (Vector Semantics):Instancia a HD, criando representações de palavras, chamadas <font color='red'>_embeddings_</font>, a partir de suas distribuições em textos. \n",
    "  - Usadas em aplicações de NLP que exploram semântica \n",
    "  - Base para representações de palavras mais poderosas (ex., ELMo e BERT)\n",
    "\n",
    "- Embeddings podem ser aprendidos de forma automática a partir de textos de entrada. Processo chamado de <font color='red'>Representation Learning</font>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf306ad1",
   "metadata": {
    "id": "bf306ad1"
   },
   "source": [
    "## Hipótese Distribucional - Exemplo\n",
    "---\n",
    "\n",
    "- O que é <font color='red'>Ongchoi</font> ?? \n",
    "\n",
    "- A palavra foi vista nos seguintes contextos:\n",
    "  - “<font color='red'>Ongchoi</font> é delicioso refogado com alho”\n",
    "  - “<font color='red'>Ongchoi</font> é excelente sobre o arroz”\n",
    "  - “... folhas de <font color='red'>ongchoi</font> com molhos salgados ...”\n",
    "\n",
    "- Algumas das  palavras nos textos acima foram vistas em contextos tais como:\n",
    "  - “... espinafre refogado com alho sobre arroz ...”\n",
    "  - “... acelga caules e folhas são deliciosos ...”\n",
    "  - “... couve e outras verduras em molhos salgados ...”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a2761",
   "metadata": {
    "id": "e83a2761"
   },
   "source": [
    "## Semântica Vetorial\n",
    "---\n",
    "\n",
    "- Modelo de semântica refinado que também permite estimar a similaridade entre palavras (ou sentenças).\n",
    "- Uma palavra é representada por um vetor ou <font color='red'>_embeddings_</font> em um espaço semântico multidimensional\n",
    "- Combina duas intuições: _hipótese distribucional_ e representação de palavras como vetores numérico. \n",
    "- Existem várias versões de semântica de vetorial, cada uma definindo os elementos dos vetores de maneiras um pouco diferentes. \n",
    "    - Em cada caso, os elementos são baseados em alguma forma de contagem ponderada de palavras vizinhas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2de80a",
   "metadata": {
    "cell_style": "split",
    "id": "1a2de80a"
   },
   "source": [
    "## Semântica Vetorial - Exemplos\n",
    "---\n",
    "\n",
    "- Projeção bidimensional de embeddings de algumas palavras.\n",
    "\n",
    "- Palavras com semânticas semelhantes estão próximas no espaço\n",
    "  - _bible_, _lord_, _church_, ...\n",
    "  - _mars_, _space_, _nasa_, ...\n",
    "  \n",
    "- As palavras próximas não são necessariamente “parecidas” sintaticamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b922317",
   "metadata": {
    "cell_style": "split",
    "id": "2b922317"
   },
   "source": [
    "## Semântica Vetorial - Exemplos\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1EpFvS9NRg4SZbog59V-YUKD-UonTDNQ2\" width=700/>\n",
    "\n",
    "\n",
    "<font size=\"2\">[Reproduzido de Li&Li (2019)](https://epubs.siam.org/doi/10.1137/1.9781611975673.77)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc4620",
   "metadata": {
    "id": "e2bc4620"
   },
   "source": [
    "## Representação de Palavras: TF-IDF x Embeddings\n",
    "---\n",
    "\n",
    "- Modelo TF-IDF: termos representados por vetores numéricos de tamanho $\\color{blue}{|V|}$ \n",
    "  - Componentes refletem a distribuição das palavras em uma coleção de documentos. \n",
    "  - Vetores longos: $\\color{blue}{|V|}$ = 20.000 a 50.000 \n",
    "  - Vetores esparsos: tipicamente a maioria das palavras ocorre em poucos documentos \n",
    "  - Sinonímia: “carro” e “automóvel” : sinônimos, mas estão em dimensões distintas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc418ed1",
   "metadata": {
    "id": "bc418ed1"
   },
   "source": [
    "## Representação de Palavras: TF-IDF x Embeddings\n",
    "---\n",
    "\n",
    "- Embeddings: Vetores <font color='red'>densos</font> e mais curtos: 50 a 1000 a elementos, a maioria com valores não-zero\n",
    "  - Mais fáceis de usar como features em ML (menos pesos para ajustar)\n",
    "  - Carregam mais semântica do que contadores de frequência\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e084ad",
   "metadata": {
    "id": "e8e084ad"
   },
   "source": [
    "## Word2Vec \n",
    "--- \n",
    "\n",
    "- Algoritmo Skip-gram with negative Sampling (SGNS) [[Mikolov et al.,2013]](https://arxiv.org/abs/1310.4546)\n",
    "  - Método pra geração de embeddings curtos e densos\n",
    "  - Incluído na library _Word2Vec_ e, portanto, é chamado comumente de <font color='red'>word2vec</font>.\n",
    "  - Rápido, eficiente para treinamento. Disponível on-line com código e embeddings pré-treinados. \n",
    "  - Outros métodos populares \n",
    "    - GloVe [[Pennington et al., 2015]](https://aclanthology.org/D14-1162/)\n",
    "    - fastText [[Bojanowski et al.,2016]](https://arxiv.org/abs/1607.04606)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e45ef3",
   "metadata": {
    "id": "a0e45ef3"
   },
   "source": [
    "## Word2Vec - Intuição\n",
    "---\n",
    "\n",
    "- Em vez de contar com que frequência cada palavra ocorre próximo a uma palavra $\\color{blue}{w}$, treinar um classificador binário para calcular a probabilidade das palavras ocorrerem próximo a $\\color{blue}{w}$  \n",
    "\n",
    " - O <font color='red'>embedding </font> é formado a partir dos pesos dos classificadores aprendidos.\n",
    " \n",
    " - A intuição revolucionária: podemos usar texto corrente como corpus de treinamento implicitamente supervisionados para esse classificador\n",
    " \n",
    " - Uma palavra $\\color{blue}{v}$ que ocorre perto de $\\color{blue}{w}$ atua como exemplo positivo\n",
    " \n",
    "- Evita a necessidade de qualquer tipo rotulamento manual\n",
    "\n",
    "- Ideia proposta inicialmente no contexto de modelos neurais de linguagem\n",
    "  [[Bengio et al.,2003]](https://jmlr.org/papers/v3/bengio03a.html) e [[Collobert et al.,2011]](https://dl.acm.org/doi/10.5555/1953048.2078186) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ced143",
   "metadata": {
    "id": "f5ced143"
   },
   "source": [
    "## Algoritmo Skip-gram – Visão Geral\n",
    "--- \n",
    "\n",
    "- Gerar um embedding $\\color{blue}{e_w}$ para uma palavra $\\color{blue}{w}$ \n",
    "  - Dado um corpus textual, gerar exemplos positivos com $\\color{blue}{w}$ e as palavras vizinhas\n",
    "  \n",
    "  - Gerar exemplos negativos fazendo amostragens aleatórias de palavras que ocorrem em um vocabulário (p.ex., um léxico) (negative sampling) \n",
    " \n",
    "  - Usar um algoritmo de aprendizagem para treinar um classificador binário que distingue os dois casos\n",
    "  \n",
    "  - Usar os pesos encontrados no treinamento para formar o embedding $\\color{blue}{e_w}$\n",
    "  \n",
    "  \n",
    "- Vários tipos de algoritmos podem ser usados, p.ex., regressão logística\n",
    "\n",
    "- Pode também ser  parte do processo de treinamento de um Modelo Neural de Linguagem. \n",
    "\n",
    "  - Objetivo é gerar o modelo e os embeddings são gerados como subproduto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644ded9",
   "metadata": {
    "id": "d644ded9"
   },
   "source": [
    "## Skip-Gram – Treinamento\n",
    "--- \n",
    "\n",
    "- Sentenças de Treino\n",
    "\n",
    "| ... | limão, | uma | <font color='orange'>_colher_</font> | <font color='orange'>_de_</font> | <font color='red'>_damasco_</font> | <font color='orange'>_picado_</font> | <font color='orange'>_uma_</font> |  pitada | ... |\n",
    "| :-: | :-:    | :-: | :-:    |:-: |    :-:  |   :-:  | :-: |  :-:    |     |\n",
    "|     |        |     |   <font color='orange'>c1</font>   | <font color='orange'>c2</font> |  <font color='red'>alvo</font> |   <font color='orange'>c3</font>   | <font color='orange'>c4</font>  |         |     |\n",
    "\n",
    "\n",
    "- Assumimos que a janela de contexto inclui duas palavras de cada lado\n",
    "\n",
    "- Dado um par $\\color{blue}{\\langle t, c \\rangle}$ = $\\langle$ $\\color{red}{alvo}$, $\\color{orange}{contexto}$ $\\rangle$ retorna a probabilidade de $\\color{blue}{c}$ ser  realmente uma palavra de contexto\n",
    "  - $\\langle$ $\\color{red}{damasco}$, $\\color{orange}{picado}$$\\rangle$\n",
    "  - $\\langle$ $\\color{red}{damasco}$, $\\color{orange}{aardvark}$$\\rangle$ \n",
    "\n",
    "\n",
    "  <center>$\\color{blue}{P(+|t,c)}$  e $\\color{blue}{P(−|t,c)=1−P(+|t,c)}$</center>      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83416e9a",
   "metadata": {
    "id": "83416e9a"
   },
   "source": [
    "## Intuição\n",
    "---\n",
    "\n",
    "- As palavras tendem a ocorrer próximas de palavras similares\n",
    "- Modelar similaridade com produto vetorial: $\\color{blue}{Similaridade(t,c) ∝ t∙c}$\n",
    "\n",
    "\n",
    "- Converter para probabilidade usando a função sigmoide:\n",
    " \n",
    "  - $\\color{blue}{P(+\\mid t, c)=\\frac{1}{1+e^{-t \\cdot c}}}$\n",
    "\n",
    "  - $\\color{blue}{P(-\\mid t, c)=1-P(+\\mid t, c)=\\frac{e^{-t \\cdot c}}{1+e^{-t \\cdot c}}}$\n",
    "\n",
    "- P/ todas as palavras do contexto $\\color{blue}{c_1,\\ldots,c_k}$, assumindo palavras independentes \n",
    "\n",
    "  - $\\color{blue}{P\\left(+\\mid t, c_{1: k}\\right)=\\prod_{i=1}^k \\frac{1}{1+e^{-t \\cdot c_i}}}$\n",
    "  \n",
    "  - $\\color{blue}{\\log P\\left(+\\mid t, c_{1: k}\\right)=\\sum_{i=1}^k \\log \\frac{1}{1+e^{-t \\cdot c_i}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e1d33",
   "metadata": {
    "id": "526e1d33"
   },
   "source": [
    "## Exemplos Negativos\n",
    "---\n",
    "\n",
    "| ... | limão, | uma | <font color='orange'>_colher_</font> | <font color='orange'>_de_</font> | <font color='red'>_damasco_</font> | <font color='orange'>_picado_</font> | <font color='orange'>_uma_</font> |  pitada | ... |\n",
    "| :-: | :-:    | :-: | :-:    |:-: |    :-:  |   :-:  | :-: |  :-:    |     |\n",
    "|     |        |     |   <font color='orange'>c1</font>   | <font color='orange'>c2</font> |  <font color='red'>alvo</font> |   <font color='orange'>c3</font>   | <font color='orange'>c4</font>  |         |     |\n",
    "\n",
    "\n",
    "- Para cada exemplo positivo, criar $\\color{blue}{k}$ exemplos negativos\n",
    "\n",
    "|     Exemplos   Positivos    |               |         |     Exemplos   Negativos (k=2)    |                |                |                |\n",
    "|:---------------------------:|:-------------:|---------|:---------------------------------:|:--------------:|:--------------:|:--------------:|\n",
    "|               _t_             |       _c_       |         |                  _t_                |        _c_       |        _t_       |        _c_       |\n",
    "|            <font color='red'>_damasco_</font>           |     <font color='orange'>_colher_</font>    |         |               <font color='red'>_damasco_</font>             |      <font color='orange'>_abade_</font>     |     <font color='red'>_damasco_</font>   |       <font color='orange'>_sete_</font>     |\n",
    "|            <font color='red'>_damasco_</font>           |       <font color='orange'>_de_</font>      |         |               <font color='red'>_damasco_</font>             |      <font color='orange'>_meu_</font>     |     <font color='red'>_damasco_</font>     |     <font color='orange'>_sempre_</font>    |\n",
    "|           <font color='red'>_damasco_</font>           |     <font color='orange'>_picado_</font>    |         |               <font color='red'>_damasco_</font>             |       <font color='orange'>_onde_</font>     |     <font color='red'>_damasco_</font>     |     <font color='orange'>_querido_</font>    |\n",
    "|            <font color='red'>_damasco_</font>          |       <font color='orange'>_uma_</font>    |         |               <font color='red'>_damasco_</font>             |     <font color='orange'>_coaxial_</font>    |     <font color='red'>_damasco_</font>    |       <font color='orange'>_se_</font>     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e1079f",
   "metadata": {
    "id": "74e1079f"
   },
   "source": [
    "## Propriedades Semânticas dos Embeddings\n",
    "---\n",
    "\n",
    "- O tamanho da janela de contexto afeta bastante os modelos de semântica vetorial. Geralmente usa-se entre 1 a 10 palavras de cada lado.\n",
    "\n",
    "- Janelas Curtas:  Tendem a representações um pouco mais sintáticas; Palavras da mesma classe gramatical\n",
    "\n",
    "- Janelas Longas: Palavras do mesmo tópico, mas não similares\n",
    "\n",
    "- Exemplo [[Levy & Goldberg, 2014]](https://arxiv.org/abs/1402.3722):\n",
    "\n",
    "  - Palavras mais semelhantes a “Hogwarts” (Escola na série Harry Potter)\n",
    "  - Janela de ± 2: Sunnydale e Evernight (Escolas em outras séries)\n",
    "  - Janela de ± 5: Dumbledore, Malfoy e Half-blood (Relacionas a Harry Potter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823c255",
   "metadata": {
    "id": "8823c255"
   },
   "source": [
    "## Analogia\n",
    "---\n",
    "\n",
    "- Embeddings tem capacidade de capturar significados relacionais.\n",
    "- Offsets dos vetores podem capturar algumas relações analógicas entre as palavras.\n",
    "\n",
    "- Exemplos:\n",
    "  - vetor (“king”) - vetor (“men”) + vetor (“woman”) = vetor (“queen”)\n",
    "  - vetor (“Paris”) - vetor (“France”) + vetor (“Italy”) = vetor (“Rome”)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32f8f8",
   "metadata": {
    "id": "dc32f8f8"
   },
   "source": [
    "## Analogia - Exemplos\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://drive.google.com/uc?id=1FHG4auEPd04PocUspU-wwb0XCEZaMpE2\" width=700/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://drive.google.com/uc?id=1FMbEKY5TMvAZKy78KxNc7vdg_F52BS5N\" width=700/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902e0d8",
   "metadata": {
    "id": "3902e0d8"
   },
   "source": [
    "## Outros Modelos - GloVe\n",
    "---\n",
    "\n",
    "- __GloVe__ [[Pennington et al., 2014]](https://aclanthology.org/D14-1162/) Embeddings são gerados considerando a co-ocorrência global de palavras em todo o corpus, enquanto o Word2vec considera a co-ocorrência dentro de um contexto local de palavras vizinhas. A matriz global de co-ocorrência de termos é computada usando técnicas de fatorização de matrizes.\n",
    "\n",
    "<center> <img src=\"https://drive.google.com/uc?id=1HGeV1fw9_l3xdjZiQOku-865TIDsAX5H\" width=600/> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6557d",
   "metadata": {
    "id": "fdc6557d"
   },
   "source": [
    "## Outros Modelos - FastText\n",
    "---\n",
    "\n",
    "- __FastText__ [[Bojanowski et al.,2016]](https://arxiv.org/abs/1607.04606) Essencialmente uma extensão do word2vec. Trata cada termo como sendo composto de n-gramas. Assim, o vetor do termo é representado pela composição dos embeddings de n-grams.\n",
    "\n",
    "- Útil em casos de palavras desconhecidas no treino.\n",
    "    - O termo “aquarium” pode ser representado pelos tri-gramas “<aq/aqu/qua/uar/ari/riu/ium/um>”. \n",
    "      -  “<” e “>” indicam o início e o fim de palavra\n",
    "    - Se o termo \"Aquarius\" é encontrado e é desconhecido, ele ainda pode ser comparado com “aquarium” comparando       os seus tri-gramas componentes \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ccbf3",
   "metadata": {
    "id": "449ccbf3"
   },
   "source": [
    "## Outros Modelos - Embeddings Contextuais \n",
    "---\n",
    "\n",
    "- Usando Modelos de Linguagem, é possível obter word embeddings contextualizados que resolvem uma grande limitação das abordagens clássicas de embedding: a desambiguação da polissemia, pois nestes modelos uma palavra com significados diferentes, como \"banco'' ou \"manga'' são representadas pelo mesmo vetor. \n",
    "\n",
    "- Assim, a palavra \"banco\" terá embeddings diferentes em \"Banco de Dados\" e \"Banco do Brasil\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aad8df",
   "metadata": {
    "id": "e1aad8df"
   },
   "source": [
    "## Outros Modelos - Embeddings Contextuais \n",
    "---\n",
    "\n",
    "- Nos embeddings contextualizados, um modelo de linguagem examina a sentença de entrada e gera um embedding para cada palavra, de acordo do contexto.\n",
    "  - ELMO [[Peters,2018]](https://arxiv.org/abs/1802.05365) - Baseado em um modelo de sequências, _Long Short Term Memory (LSTM)_ bidirecionais\n",
    "  - BERT [[Devlin,2018]](https://arxiv.org/abs/1810.04805v2) -  Baseado no modelo _Bidirectional Encoder Representations from Transformers_ que substitui o LSTM por mecanismos de _atention_\n",
    "\n",
    "- Veremos mais detalhes mais à frente no curso\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
